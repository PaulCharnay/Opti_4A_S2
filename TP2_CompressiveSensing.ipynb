{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHARNAY Paul - MONEDIERES Emmeline\n",
    "\n",
    "TP 2 - Echantillonnage compressif\n",
    "=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import math\n",
    "import scipy.fftpack as fft\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récemment (début années 2004-présent), de nouveaux concepts et théorèmes ont été développés et risquent de \n",
    "révolutionner à relativement court terme la fabrication de certains appareils de mesure numériques (microphones, imageurs, analyseurs de spectres,...). \n",
    "Ces nouvelles techniques sont couramment appelées échantillonnage compressif, \"compressive sampling\" ou encore \"compressed sensing\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Le théorème de Shannon"
   ]
  },
  {
   "attachments": {
    "EchantillonageSous_Nyquist.png": {
     "image/png": "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"
    },
    "Echantillonnage09.png": {
     "image/png": "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"
    },
    "Echantillonnage105.png": {
     "image/png": "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"
    },
    "Echantillonnage201.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGkCAIAAACgjIjwAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH2gsLDR8HhEYrlwAAACJ0RVh0Q3JlYXRpb24gVGltZQAxMS1Ob3YtMjAxMCAxNDozMTowNj6aCMoAAAAkdEVYdFNvZnR3YXJlAE1BVExBQiwgVGhlIE1hdGh3b3JrcywgSW5jLrrEUs8AABJlSURBVHic7d3RmqI4EAZQ2G/e/5WzF3QzjCIiAqlKzrnYddXViJg/VaA9llIGAKjtv9oDAIBhEEgABCGQAAhBIAEQgkACIASBBEAIAgmAEAQSACEIJABCEEgAhCCQAAhBIHHE+GT7zl8+19sLoWxvk1e3Plwz/efyyrcPuzGenSPf46ONv+fOMd9EqvhTewBkdf/P8qb4IeBxHOdxLi9v3Lo6I5dSXt159WH3jOdcex52532uGyS5qJA42UYF8FwEPFw5lwUPK+vncmHP8z4/2qsrXw17vmZ/pXJgYi2lPP9f0xy9/+XvsfEaX73w7UdbXlgthlbHv70B6ZkKiTO9qg+WC/yHC9v33Ll83v+8+wezWsG8eqLtIR0w/b/zP5ePtlo8rc7vJ26WbcsxPL9ly2tWn1GRxEQgcdDDDPg8mzxMSRu37rn+mNVH2z+Y+dZPl/PBp9dltu2521kPCNsEEgd9P/WEatrsH8yeXtbV8/JcUnz/XM8vZ1nrfPPI+58RJgKJOh5aN4kGsz1NB6+NHrx64Z+27D6SaPtwMyc1cJXqMXOWVy+k7gv8pjzaHvltr6uZPYSzqJA4aPUY0vJwy/4DM2/n1rcz7/7n3T+Yh6P0+59oo9T4cpzfWH3qty/8xAE8PPjpj08DMrUXoJZcjbilFCNPMUhuoGUH6zSU4GYWJvBSG82l4PVH8OFxJ7sCACFo2QEQgkACIASBBEAIAgmAEFoIJKfnAjQg9y81iCKAZuSukKZfIqk9CgBOkLtC2vb2D/Zs/r9DKT//7JaNMO9B3W6BwUYYhsFn4S4tB9Lw9Rfse94F5xfe80YYfmfhnrfAYCP86vyzcIPcLbuLLPe5aRfszcOnrs+NsNTtFvBZGGyEGwmkR1ZATKTysw43ggnhTgIJVpiGBhuB2wmk93pbFZqGYENvE8KdWgikE8/8Nhe/4kM42Aj9MSHcrIVAAk73ai6WylxHIMEj62K2SeWLCKS/NqahfvY/GwGoRSDBB6RyPxTK9xNIwKPtuVgqcxGBBP+wLmYPqXwFgbRXD/ufuRioSCD9MBfv0UMqA7UIJPiMVO6BFWoVAgn4x565WCpzBYEEf1kXs59UPp1A+kDb+5+5GKhLIA2DufgTbacyUJFAgo9J5bZZodYikIAjpDKnE0jAX4qDj0jlcwkk+GEuhroE0mdaXRCZi4HqBBIfazWVgboEkuKAI6QynE4gAfxlhVqRQAJ+fDoXKxM5l0ACOE4qn0ggfcz+1ySNGqjuT+0BvDf+Tv9lbcLYvpU9zMVABNEDaRzHOWmWl/fcuu/xzcVHTGWiTQecSMsODtK8hXNFr5C2lVJ2NvRe3QFgpu6vK3cgvW3ZCSG4lOYtJ9KyA4ZBcfAFzduzCCQwF0MIAukICyKA00U/hrR62sJ8uOjtSQ28dbg4cPAAOFf0QBrWkmZ5zTc5ZD4FiEPLDo7TvIUTCSTgK82kspZJdQIJMBcTgkAC+FYzZWJdAoneKQ4gCIF0kAURwLkEUte+LA6kMnCifgNJowYglH4DCU6hTISzCCQALZMQBBLwLWUipxBI0DvFwSmk8vcEEgAhCKTjLIgaoDiAOAQSACEIpH6dUhwoE4GzdBpIGjUA0XQaSHAiZSKcQiABEIJAAk6QukzUww9CIEHXzMXEIZAAzpG6TIxAINEvxQGEIpC+YkEEcJY/tQfw3vg75Ze11ez2rbyiOACiiR5I4zjOSbO8vOdW7jGVibY98KV2Wnb708jsCRBQ9Appj6lrt93Qe3UHOIUyEb6XPpDmTt1qy04IAdusJOJI37ITORCEk075UvpAAg5THJxOKn9DIAEQQvRjSKWU528azYeLVm+FPRQHEE30QBrWkmZ5TfUccnoVwCm07HokQYGABBIncCAX+F53gaQ4AIipu0CCiygT4UsCCeiXlkkoAgk4jTKRbwgk6JTigGgEEsCZlImHCSR6pDiAgATSCSyIAL4nkAAIQSB156JulTIR+JJAAiCEvgLJoWwupUyEb/QVSACEJZCAMyUqE7VMohFIAIQgkKBHigMCEkgAJ0vUtwxFINEdxQHEJJDOYUEE8CWBBEAIAqkvl3arlInANwQSACEkCKTx1/Z9djyOQ9lcTpkIh/2pPYA3xnEsvzGyvPxwn3sHBcD5ElRI216lFFBLijJRyySg6BXSl/4tnux9AHHlDqS35dHy1vhLNriH4oCYErfsNOuAsFL0LaNJXyEtL8sn3lIcQFiJA+nfdlz9NJoWRLVHAZBV9EAqpcxl0NvzvwHIK3ogDf9WQvuv4dkNBZwyETgs8UkNALSkl0CybAcIrpdAgts43xeOEUgAhCCQgPMFLxP18GMSSACEIJCgL4oDwhJIAJcI3rcMSCDREcUBRCaQzmRBBHCYQAIgBIHUi9u6VcpE4BiBBEAIXQSSQ9ncTJkIB3QRSADEJ5CAvmiZhCWQgEvoW/IpgQRACAIJOqJbdTNl4kcEEgAhCCR6oTiA4ATSyVToAMcIJABCEEhduLlbpUwEDvhTewDvjb9zW1mbU7dvBSCL6IE0juOcNMvLe279vd6hbCqYykT7HuyXu2WnKgJoRu5Amr0uj8bpn6NjGoCWSWwtBNKrNBp+S6hSiloK7uf0Fj6SPpA20giARHIHkjSC/XSrqlAm7pc7kABoRvTTvkspz980ejjbe3nnm4dHFooDiC96IA1rMTNfEzOBfAEF4AAtOwBCEEjtq1KuOZALP8axDKPPwx4CCeAy4ziUMg7FGm2PxgPJsRygmocJSCa903ggQUXmH/iIQAIgBIEEXOjneH6MUvHuHv5DjewQwjsCCbiMQ/ql/D3LThq9I5CgCxXmQ4f0J3Mk845AArhWt1n8KYEEQAgC6RIWRKFU6953/hV9h/T5kECCaziePzikz2cEUuPMA3U4nj9zSJ/dBBJX6XkSBg5oO5Bi/r0kAFa0HUhU1fMh/d/y8OffOqewQ4K/GEtK0yH9aR7uczqeM2m6DLyjQuICDulPSvk5pN+3IO9/n+uiXAQSACEIJGif4oAUBBIX8BV9+FeQvmVwbZ/UMA712vdlGIdp/+tzLv75iv70JnS5BYAPqZCu4Wdjhihf0VeeQRYC6QLOMQP4XPqW3fg71z/+KsNPv6jr5XHfrx5IJneFNI5j+TU+HUVffFGeOmx+YL/cgbSuesfMOWYAn2sxkBbGcZwqp7FGJvkzMAzKRMIYw/+0ZPpjSNt+DiyNY4Xf/Z5/yQ0ghuAzUosVko7ZpOcf2yaY6mVit9NALi0G0rD8Vmavu6EvQgHZNBpIw/D3LLsOVT+tA+BzuY8hLc/29tdhYVW3bYJoppWh92JD7kAa5BBAK9pt2fXMaR2Tcfw5swPIQCBdqOaxG1+EcloHZCOQ2hXjx7brcFoHJCSQmhWkNJIFwE4CCYAQBBItcloH/CvFh0Ag0ahIp3XU7Fv6BSnyEEi0q+fTOiZONSQVgQSNCnaqYcXnD1Aks4tAAiAEgQQtUxyQiECCRjnVcBLptI7afdPoBBK0q5SfubjjNHJaRyIC6Vo+BRV1Owkv/czFfW6IYKd18JZAAiAEgdQmxQGQjkDicjol1OG0jkmePwyW/i/GArz08wtSwzAO/aZRKT//Dh/JKiS4gzKxmp5/QWqZRiXBXiiQAAhBIAE3qbJAD9+m4i+BBNCobKd1OKkBmhV+/uF6qU7rUCEB3KdC3zLPaR0JKqTx990raxt0+1YAsogeSOM4zkmzvLzn1iCmBVHIobXMNod0crfsYiYQAAdEr5B2elUejYtmbT/ppTgAMmohkDaadf2EUHD6lsBbsQLpQEET9tARAB+JFUifRos0IhFlIlUk2utyn9QAQDNiVUjPSinP3zR6ONt7eeebhwdElqg4YIgfSMNazMzXSCDIRd+SDVp20CbzPukIJIBbhf87edUIJABCEEh3sCC6mW4VZCSQAAhBILVGcQAkJZC4ib4lsE0gARCCQIL7KBO5Wa4evkACIASBBLQpV3HAIJCAm+lb8opAggYpDshIIAHcTZm4SiABEIJAuokF0W10qyApgQRACAKpKYoDIC+BxH30LYENAgmgTelaJgIJbqVMhFcEEgAhCCTgbjeUiem6VQwCCYAgEgTS+Gv7PreNB4JTHKTgaOKzP7UH8MY4juX3s7W8/HCfewcFwPkSVEjbXqVUQBZEN1AcQF7RK6QvLYunLLkF0KfcgfS2POoqhBQHQGqxAumjgiZRs47Z1Lf0vgHPYgXSpwGzDDD5BJBarED6yDJ+pBGJKBO5QcZ9LHoglVLmMujt+d8A5BU9kIa1Pt6ea4BuZSwOGBr4HhKQka/l8UwgQVMUB+QlkADqUCY+EEi0Q3EAqQmkW1kQAbwikBqhOACyE0jcTZkIrBJIAIQgkKACZSKXStrDF0gAhCCQgKYkLQ4YBBJQi74lDwQStENxQGoCCaAaZeKSQLqb/e8iigPITiABEIJAaoHiAGiAQKICfUvgmUACaErelolAgjqUifBAIAHtyFscMAgkoCJlIksCCYAQBBI0QrcqKWXi7E/tAbw3/r5XZe3Ttn0rnTAXQwOiB9I4jnPSLC/vuTWsaUGUZLAAN2mnZZcljU4n24A2RK+Q9pi6dtsNvVd3oBZlIvAgfSDNnbrVlp0QArqSep0XK5AOFDQih7yUibAUK5CkC3CYdM+unZMagIx8C4dZrArpWSnl+ZtG8+Gi1VuhQ4oDGhA9kIa1pFleI4eA7BxNnGjZkZ5PMrRBINWhbw7wQCDlpjgAmiGQqEaZCCwJJKhJKnOi7C0TgQRACAIJqOyUMjF7ccAgkAAIQiBBeoqDBjiaOAgksjMXQzMEUjUWRABLAikxxQHQEoFETcpEYCaQoDKpzCkaaJkIJCC9BuZiBoEERKBMZBBIkJ3igGYIJIAQlIkCicQUB9ASgVSTBRHATCBl1UxxIJWBiUACSK+NFapAgvqUiTAIJCC7NooDhmH4U3sA742/S8eyttNt3wpta2kunsrEZl4OB0QPpHEc56RZXt5zK0Aunaeylh1Z9fy5hSZFr5C2lVJ2NvRe3aG6zhdEALPcgfS2ZRczhL7XWIZJZWCIFkjxCxqAaJpZz8UKJCFEt5SJx9hoLXFSAxCFLwh3LlaF9Gz1tIX5cNHbkxqgYYqDJvVcK0cPpGEtaZbXyKE+dfuJhYZp2dWnTQEwCKSMmiwOpDIgkAASa2mFKpAgCmXip1qaixkEEiTV6lwslXsmkMin1bkYJt2mskAKodv9D2AmkJJpuDiQytA5gQSQVWMrVIEEgSgT92tsLmYQSJBR23OxVO6WQCKZtudimPSZygIpij37n7kYaJhAIpA+V4VwTHsrVIEEsUjlPdqbixkEEqTTw1wslfskkMikh7kYJh2mskAKZHv/MxcDbRNIxNLhqhAOaHKFKpAgnI1UbnIaWmUjdEggkYZpiN701jAQSLG82v/MxcCs1QlBIBFOb6vCVTbCK63OxQwC6VJj7Rml+gBOHMPhachGOHEAhx0bwLmpnHQjtDeGDQIpHEtjYNbVhNBCIAXP/O912KPo6kP4io3A5GFPaHhCyB1I4zg2mUbL/a/hnW+bjTDYCD3NxTu1vQX+1B7AV0opQ7sV0vSyGt759rARhu43wjKTut0IQx+7wVjyv75xXH8VrQYVwGGR5/zcFdK2yNsdgAeZAmlZ8QgbgMZkCiQhBNCw3GfZAdAMgQRACC2cZQdAA1RIAIQgkAAIIdNZdh+ZzxG/rSf59hlffYH3ngHcs0Gqj2HPU1R8Ix6+rH3RMCK/C/dsge0xvL21jQGsunrn/1KbgbTc6Pe8AdvPeMNvRrwdwA0bpPoY9jzF1e/F2zFcvTcGfxeW/3ndexF8I9w/QQ1JfrlGy+4OpZS6q5IIa6IIYwi+PLxZ3U3hvbhZ9VlojzYrJF6pPgtMy7T4H4zr1OrVPI+h23ehlBLhXeCZQOpI9TQaFj/Qfv9I4rz8Sa3xzM9bcYPUfS+qdMyWlonIkkDqRYTpuKLOX/6S7RDBMhHrjiQUx5C6EGE6rv7BG39VHEz1jQCRqZDoQoReGRPb3xZ4pdntEuF7SA+73f1ff1keLVje887vXjz06+sO4NU1d46h+kaIMIB7vomxPYaHW9sbwKtRRZ7zQw8OgH44hgRACAIJgBAEEgAhCCQAQhBIAIQgkAAIQSABEIJAAiAEgQRACAIJgBAEEgAhCCQAQhBIAIQgkAAIQSABEIJAAiAEgQRACAIJgBAEEgAhCCQAQhBIAIQgkAAIQSABEIJAAiAEgQRACAIJgBAEEgAh/A9rRLw6AgIOMQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aujourd'hui, presque tous les appareils de mesure reposent sur le théorème de Shannon. Celui-ci (vous l'avez déjà vu en 2ème année) peut s'énoncer ainsi : \n",
    "> Soit $g:\\mathbb{R}\\to \\mathbb{R}$ une fonction de $L^2(\\mathbb{R})$. Si sa transformée de Fourier $\\hat g$ a un support contenu dans l'intervalle $[-f_M, f_M]$, alors en l'échantillonnant à une fréquence d'échantillonnage $f_e\\geq 2f_M$, on peut la reconstruire exactement.\n",
    "\n",
    "Ce théorème est illustré sur les figures ci-après:\n",
    "![Echantillonnage105.png](attachment:Echantillonnage105.png)![Echantillonnage201.png](attachment:Echantillonnage201.png)\n",
    "![Echantillonnage09.png](attachment:Echantillonnage09.png)![EchantillonageSous_Nyquist.png](attachment:EchantillonageSous_Nyquist.png)\n",
    "\n",
    "Les instruments de mesures qui reposent sur ce théorème sont donc construits suivant le principe : \n",
    ">Filtre passe-bas $\\rightarrow$ Echantillonnage à une fréquence $f>2f_M$ $\\rightarrow$ Interpolation sinc\n",
    "\n",
    "Pour beaucoup d'applications, ce principe présente deux défauts majeurs :\n",
    "* Les signaux sont rarement naturellement à spectre borné, et on perd donc l'information haute-fréquence en effectuant un filtrage passe-bas.\n",
    "* Pour beaucoup de signaux, il faut choisir une très haute fréquence d'échantillonnage pour obtenir un résultat satisfaisant. \n",
    "Ceci implique que les données à stocker ont une taille très importante et qu'il faut les compresser après coup (par exemple : jpeg).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L'échantillonnage compressif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Principe général**\n",
    "\n",
    "L'idée sous jacente à l'échantillonnage compressif est de réaliser la compression dès l'acquisition.\n",
    "Supposons que le signal $x\\in \\mathbb{R}^n$ que l'on souhaite mesurer s'écrive comme une combinaison linéaire de la forme :\n",
    "\\begin{equation}\n",
    "(1)~~~~~~~~~~~ x=\\sum_{i=1}^m\\alpha_i \\psi_i\n",
    "\\end{equation}\n",
    "où $\\psi_i\\in \\mathbb{R}^n, \\ i=1..m$, sont des \"fonctions de base\" (en traitement d'images, ces fonctions pourraient être des ondelettes, en traitement du son, des ondelettes ou des atomes de Fourier, pour certaines applications, on pourrait imaginer des splines...} et $\\alpha_i\\in \\mathbb{R}$ sont des coefficients. \n",
    "On peut réécrire l'équation (1) sous la forme matricielle condensée :\n",
    "$$\n",
    "x=\\Psi \\alpha \\ \\ \\textrm{où } \\ \\ \\alpha=\\begin{pmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_m \\end{pmatrix}\\ \\ \\textrm{et} \\ \\ \\Psi=\\begin{pmatrix} \\psi_1,\\psi_2,..., \\psi_m\\end{pmatrix}.\n",
    "$$\n",
    "Pour pouvoir reconstruire tous les éléments de $\\mathbb{R}^n$, on suppose généralement que la matrice $\\Psi$ est une matrice surjective (ainsi, la famille  des $(\\Psi_i)_i$ est génératrice), ce qui implique que $m\\geq n$. Dans le langage du traitement d'image, on dit alors que $\\Psi$ est un frame (une base si $m=n$).\n",
    "\n",
    "L'échantillonnage compressif repose sur l'hypothèse suivante : les signaux $x$ que l'on souhaite mesurer sont parcimonieux, \n",
    "c'est-à-dire que la majorité des coefficients $\\alpha_i$ dans (1) sont nuls ou encore que \n",
    "$$\\#\\{\\alpha_i\\neq 0, i=1..m\\}\\ll n.$$\n",
    "On va voir que cette hypothèse permet - dans certains cas - de réduire drastiquement le nombre de mesures par rapport au théorème de Shannon avec en contre-partie, le besoin de résoudre un problème d'optimisation pour reconstruire la donnée. L'objectif de ce TP est de résoudre le problème d'optimisation résultant.\n",
    "\n",
    "Le principe de l'acquisition du signal $x$ est le suivant :\n",
    "\n",
    "- On effectue un petit nombre $p\\ll n$ de mesures linéaires du signal $x$ inconnu. On note ces mesures $y_i$, et comme elles sont linéaires par rapport à $x$, il existe pour chaque $i$ un vecteur $a_i\\in \\mathbb{R}^n$ tel que \n",
    "$$y_i=\\langle a_i, x\\rangle, i=1..p.$$ On peut aussi écrire cette opération de mesure sous la forme condensée :\n",
    "$$\n",
    "y=Ax\\ \\ \\textrm{où } \\ \\ y=\\begin{pmatrix} y_1 \\\\ \\vdots \\\\ y_p\\end{pmatrix} \\ \\ \\textrm{et} \\ \\ A=\\begin{pmatrix} a_1^T\\\\a_2^T\\\\ \\vdots\n",
    "\\\\ a_p^T\\end{pmatrix}.\n",
    "$$\n",
    "- On reconstruit le signal $x$ en résolvant le problème contraint suivant :\n",
    "\n",
    "$$\n",
    "(2)~~~~~~~~~~~ \\mbox{Trouver } \\alpha^\\star \\mbox{ solution de: }\\displaystyle\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_0\n",
    "$$\n",
    "\n",
    "où $\\|\\cdot\\|_0$ est la norme de comptage, aussi appelée norme $l^0$ définie par : \n",
    "$$\n",
    "\\|\\alpha\\|_0=\\#\\{\\alpha_i\\neq 0, i=1..m\\}.\n",
    "$$\n",
    "Autrement dit, l'idée est la suivante : on  cherche $\\alpha^\\star$, le signal le plus parcimonieux dans le frame $\\Psi$, parmi les signaux qui peuvent donner lieu aux mesures $y$. \n",
    "Après avoir trouvé $\\alpha^\\star$, on recouvre $\\tilde x$, une approximation du signal $x$ en calculant $\\tilde x=\\Psi\\alpha^\\star$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Simplification du problème d'optimisation**\n",
    "\n",
    "Le problème précédent est un problème combinatoire NP-complet, ce qui signifie que trouver $\\alpha$ peut demander un temps exponentiel en fonction de $n$, la dimension du signal. Pour le résoudre en pratique, il est souvent remplacé par : \n",
    "\n",
    "$$\n",
    "(3)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\displaystyle\\arg\\min_{\\alpha \\in \\mathbb{R}^m, A\\Psi\\alpha=y} \\|\\alpha\\|_1\n",
    "$$\n",
    "\n",
    "où $\\|\\alpha\\|_1=\\sum_{i=1}^m|\\alpha_i|$ est la norme $l^1$ de $\\alpha$. On peut dans certains cas montrer que les solutions de (2) et de (3) sont identiques. \n",
    "\n",
    "Un appareil de mesure n'étant jamais parfait, il est impossible de mesurer exactement $y_i=\\langle a_i, x\\rangle$. \n",
    "Le vecteur $y$ est bruité et la contrainte $A\\Psi\\alpha=y$ est trop forte. Elle est donc généralement relaxée et le problème devient : \n",
    "\n",
    ">$$(4)~~~~~~~~~~~  \\mbox{Trouver } \\alpha^*\\in \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.$$\n",
    "\n",
    "Si $\\sigma$ tend vers $0$, la solution du problème (4) tend vers une solution du problème (3). C'est le problème (4) que nous allons résoudre dans ce TP. Dans la suite , on notera $F$ la fonction :\n",
    "$$\n",
    "F(\\alpha)=\\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.\n",
    "$$\n",
    "\n",
    "Pour conclure cette introduction à l'échantillonnage compressif, notons que de façon similaire au théorème de Shannon, on dispose d'une condition de reconstruction exacte :\n",
    "\n",
    "> Supposons que :\n",
    "* $x=\\displaystyle\\sum_{i=1}^m\\alpha_i\\psi_i\\in \\mathbb{R}^n$ avec $\\|\\alpha\\|_0=k$.\n",
    "* On effectue $p$ mesures linéaires de $x$ avec $p\\geq C \\cdot k \\cdot \\log(n)$, où $C=20$.\n",
    "* On choisit les coefficients de la matrice $A\\in \\mathcal{M}_{p,n}$ de façon **aléatoire** (e.g. on peut choisir les coefficients $a_{i,j}$ de $A$ de façon indépendante suivant une loi normale.)\n",
    "\n",
    "> Alors, la résolution du problème (3) permet de reconstruire $x$ **exactement** avec une très grande probabilité \n",
    "\n",
    "L'expérience a montré qu'en pratique, il suffit en général de $p=2k$ mesures pour reconstruire le signal exactement en grande dimension !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Préliminaires théoriques\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par remarquer que les problèmes (3) et (4) sont convexes (contraintes convexes et fonctions convexes) tandis que le problème (2) ne l'est pas. En revanche, aucun des trois problèmes n'est différentiable.\n",
    "\n",
    "**Q1.** Soit $J(\\alpha) =\\frac{\\sigma}{2}\\|A\\Psi \\alpha - y\\|_2^2$. Calculez $\\nabla J(\\alpha)$.\n",
    "\n",
    "$$\\nabla J(\\alpha) = \\sigma \\Psi^\\top A^\\top \\left(A\\Psi \\alpha - y\\right) $$\n",
    "\n",
    "\n",
    "**Q2.** Montrer que la fonction $J$ est de classe $C^1$ à gradient Lipschitz et calculer un majorant $L$ de la constante de Lipschitz de $\\nabla J$ en fonction de $|||A|||$, de $|||\\Psi |||$ et de $\\sigma$.\n",
    "    \n",
    "$J$ est continue et $\\nabla J$ existe et est continue donc $J$ est de classe $C^1$. \n",
    "De plus, $\\nabla J$ est différentiable donc par le théorème des accroissements finis,\n",
    "$$\n",
    " \\forall (x,y), \\| \\nabla J(x) - \\nabla J(y) \\| \\leq |||H_J(u)||| \\times \\|x - y\\| $$\n",
    " \n",
    " avec \n",
    "\n",
    "$$ |||H_J(u)||| = \\sigma |||A|||^2 \\times |||\\Psi|||^2 $$\n",
    "\n",
    "\n",
    "Donc $J$ est à gradient Lipschitz.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construction de l'algorithme**\n",
    "\n",
    "On note $\\alpha^k$ l'itéré courant. En appliquant le lemme de Nesterov à la fonction $J$, on a:\n",
    "$$\\forall \\alpha\\in \\mathbb{R}^m,~J(\\alpha)\\leq {J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha-\\alpha^k\\rangle + \\frac{L}{2}\\|\\alpha-\\alpha^k\\|_2^2}.\n",
    "$$\n",
    "\n",
    "En posant $\\phi(\\alpha,\\alpha^k)=J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha-\\alpha^k\\rangle + \\frac{L}{2}\\|\\alpha-\\alpha^k\\|_2^2 +\\|\\alpha\\|_1$, on a alors:\n",
    "\n",
    "$$\\forall \\alpha\\in \\mathbb{R}^m,~F(\\alpha) = J(\\alpha)+\\|\\alpha\\|_1\\leq \\phi(\\alpha,\\alpha^k),$$\n",
    "\n",
    "avec : $\\phi(\\alpha^k,\\alpha^k) = F(\\alpha^k)$.\n",
    "\n",
    "Cette inégalité motive alors l'algorithme de descente suivant:\n",
    "\n",
    "$$\\alpha^{k+1}=\\displaystyle\\arg\\min_{\\alpha\\in \\mathbb{R}^m} \\phi(\\alpha,\\alpha^k).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** Montrer que l'algorithme s'écrit de façon équivalente sous la forme:\n",
    "$$\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right).$$\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "\\alpha^{k+1} &= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\Phi(\\alpha, \\alpha^k) \\\\\n",
    "&= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} J(\\alpha^k) + \\langle \\nabla J(\\alpha^k), \\alpha - \\alpha^k \\rangle + \\frac{L}{2} \\| \\alpha - \\alpha^k \\|_2^2 + \\| \\alpha \\|_1 \\\\\n",
    "&= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} J(\\alpha^k) + \\frac{L}{2} \\left( \\| \\alpha - \\alpha^k \\|_2^2 + \\frac{2}{L} \\langle \\nabla J(\\alpha^k), \\alpha - \\alpha^k \\rangle \\right) + \\| \\alpha \\|_1 \\\\\n",
    "&= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} J(\\alpha^k) + \\frac{L}{2} \\| \\alpha - \\alpha^k  + \\frac{1}{L} \\nabla J(\\alpha^k) \\|_2^2 - \\frac{1}{L^2} \\| \\nabla J(\\alpha^k) \\|_2^2 + \\| \\alpha \\|_1 \\\\\n",
    "&= \\arg\\min_{\\alpha \\in \\mathbb{R}^m}\\| \\alpha \\|_1 + \\frac{L}{2} \\| \\alpha - \\alpha^k  + \\frac{1}{L} \\nabla J(\\alpha^k) \\|_2^2  \\\\\n",
    "& \\text{car $\\alpha^k$ et donc $J(\\alpha^k)$ sont constants, ie ne dépendent pas de $\\alpha$}\n",
    "\\end{align}$$\n",
    "\n",
    "Or, \n",
    "$$\\begin{align}\n",
    "\\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right) &= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\frac{1}{L} \\| \\alpha \\|_1 + \\frac{1}{2} \\| \\alpha - \\alpha^k + \\frac{1}{L} \\nabla J(\\alpha^k) \\|_2^2 \\\\\n",
    "&= \\arg\\min_{\\alpha \\in \\mathbb{R}^m} \\| \\alpha \\|_1 + \\frac{L}{2} \\| \\alpha - \\alpha^k + \\frac{1}{L} \\nabla J(\\alpha^k) \\|_2^2 \\quad \\text{ puisque $L \\geq 0$}\n",
    "\\end{align}$$\n",
    "\n",
    "On a donc bien l'équivalence entre ces deux problèmes.\n",
    "\n",
    "**Q5.** De quelle quantité la fonction coût $F(\\alpha)$ décroît-elle à chaque itération ?\n",
    "\n",
    "\n",
    "On a vu qu'on pouvait écrire $F$ comme la somme de deux fonctions : $F(\\alpha) = J(\\alpha) + \\|\\alpha\\|_1$ .\n",
    "\n",
    "$J$ est une fonction convexe à gradient L-Lipschitz, c'est à dire que $\\forall (x,y) \\in \\mathbb{R}^n \\times \\mathbb{R}^n $ :  \n",
    "$$J(y) \\leq J(x) + \\langle \\nabla J(x), y-x \\rangle + \\frac{L}{2} \\|y-x\\|^2_2$$  \n",
    "\n",
    "Dans notre cas, en prenant $y = \\alpha_{k+1}$ et $x = \\alpha_k$, on obtient\n",
    "$$\\begin{align}\n",
    "& J(\\alpha_{k+1}) \\leq J(\\alpha_{k}) + \\langle \\nabla J(\\alpha_{k}), \\alpha_{k+1}-\\alpha_{k} \\rangle + \\frac{L}{2} \\|\\alpha_{k+1}-\\alpha_{k}\\|^2_2 \\\\\n",
    "\\Leftrightarrow & J(\\alpha_k) \\geq J(\\alpha_{k+1}) + \\langle \\nabla J(\\alpha_k), \\alpha_k - \\alpha_{k+1} \\rangle - \\frac{L}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2\\\\\n",
    "\\Leftrightarrow & \\frac{1}{L} J(\\alpha_k) \\geq \\frac{1}{L} J(\\alpha_{k+1}) + \\frac{1}{L} \\langle \\nabla J(\\alpha_k), \\alpha_k - \\alpha_{k+1} \\rangle - \\frac{1}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2 \\text{ : notée $(E1)$}\n",
    "\\end{align}$$\n",
    "\n",
    "On pose $h(\\alpha)=||\\alpha||_1$. $h$ est une fonction convexe sci.\n",
    "On sait que \n",
    "$$\\begin{align}\n",
    "\\alpha_{k+1} &= \\mbox{prox}_{\\frac{1}{L}h}\\left(\\alpha_k - \\frac{1}{L}\\nabla J(\\alpha_k)\\right) \\\\\n",
    "&= \\arg \\min_{y \\in \\mathbb{R}}\\frac{1}{L}h(y)+ \\frac{1}{2}||y - \\alpha_k + \\frac{1}{L} \\nabla J(\\alpha_k)||^2_2\\\\ \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "La condition d'optimalité de ce problème s'écrit :\n",
    " \n",
    "$$\\begin{align}\n",
    "& 0 \\in \\left\\{\\frac{1}{L}\\partial h(\\alpha_{k+1}) + \\alpha_{k+1} - \\alpha_k + \\frac{1}{L}\\nabla J(\\alpha_k)\\right\\} \\\\\n",
    "\\Leftrightarrow & \\alpha_{k+1}-\\alpha_k +\\frac{1}{L}\\nabla J(\\alpha_k) \\in \\frac{1}{L}\\partial h(\\alpha_{k+1}) \\\\\n",
    "\\Leftrightarrow & \\frac{1}{L} h(y) \\geq \\frac{1}{L} h(\\alpha_{k+1}) + \\langle \\alpha_k - \\frac{1}{L}\\nabla J(\\alpha_k)-\\alpha_{k+1} , y - \\alpha_{k+1} \\rangle , \\forall y \\in \\mathbb{R}^n \\text{, par définition du sous-différentiel.}\n",
    "\\end{align}$$\n",
    "  \n",
    "Si on pose $y=\\alpha_k$, cette inégalité devient :\n",
    "$$\\begin{align}\\frac{1}{L} h(\\alpha_k) &\\geq \\frac{1}{L} h(\\alpha_{k+1}) +\\langle \\alpha_k - \\frac{1}{L}\\nabla J(\\alpha_k) - \\alpha_{k+1}, \\alpha_k - \\alpha_{k+1} \\rangle\\\\\n",
    "&\\geq \\frac{1}{L} h(\\alpha_{k+1}) - \\frac{1}{L}\\langle \\nabla J(\\alpha_k), \\alpha_k - \\alpha_{k+1}\\rangle + \\|\\alpha_k - \\alpha_{k+1}\\|_2^2 \\text{ : notée $(E2)$}\n",
    "\\end{align}$$  \n",
    "\n",
    "\n",
    "En sommant $(E1)$ et $(E2)$, on obtient :   \n",
    "\n",
    "$$\\begin{align}\n",
    "&\\frac{1}{L} J(\\alpha_k) + \\frac{1}{L} h(\\alpha_k) \\geq \\frac{1}{L} J(\\alpha_{k+1}) + \\frac{1}{L}h(\\alpha_{k+1}) +\\frac{1}{L} \\langle \\nabla J(\\alpha_k), \\alpha_k - \\alpha_{k+1} \\rangle - \\frac{1}{L} \\langle \\nabla J(\\alpha_k), \\alpha_k - \\alpha_{k+1} \\rangle + \\|\\alpha_{k+1} - \\alpha_k\\|^2_2 - \\frac{1}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2\\\\\n",
    "\\Leftrightarrow &\\frac{1}{L} F(\\alpha_k) \\geq \\frac{1}{L} F(\\alpha_{k+1}) + \\frac{1}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2 \\text{ , puisque $F = J +h$}\\\\\n",
    "\\Leftrightarrow & F(\\alpha_k) \\geq F(\\alpha_{k+1}) + \\frac{L}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2 \\\\\n",
    "\\Leftrightarrow & F(\\alpha_k) - F(\\alpha_{k+1}) \\geq  \\frac{L}{2}\\|\\alpha_{k+1} - \\alpha_k\\|^2_2 \\geq 0\n",
    "\\end{align}$$\n",
    "\n",
    "On voit donc clairement que  $F(\\alpha_k) \\geq F(\\alpha_{k+1})$ ce qui signifie que la fonction coût décroit à chaque itération : c'est bien un algorithme de descente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Partie expérimentale\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, on va chercher à reconstruire un signal unidimensionnel $x~:~[0,1]~\\rightarrow~\\mathbb{R}$ de la forme :\n",
    "$$\n",
    "x(t)= \\alpha_{k}\\delta_{k/n}(t)+\\sum_{k=1}^n\\alpha_{k+n}\\cos\\left(\\frac{2k\\pi}{n} t\\right)\n",
    "$$ \n",
    "on a donc $m=2n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialisations\n",
    "def Init(n, p, plot=False):\n",
    "    t= np.linspace(0, 1, n) #On definit un signal sur [0,1]\n",
    "\n",
    "    ## Generation du signal\n",
    "    x = np.zeros(n)\n",
    "    tmp = np.zeros(n)\n",
    "    #On ajoute deux cosinus\n",
    "    tmp[350] = 4\n",
    "    x += fft.idct(tmp, norm='ortho')  \n",
    "    tmp = np.zeros(n)\n",
    "    tmp[150] = -3  \n",
    "    x += fft.idct(tmp, norm='ortho')\n",
    "    #On ajoute deux diracs\n",
    "    x[int(n/3)] = 0.5    #Tester 0.5\n",
    "    x[int(2*n/3)] = -1 #Tester -1\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(t, x, 'b')\n",
    "        plt.show()\n",
    "    ## Mesure du signal\n",
    "    A = np.random.randn(p, n) #La matrice de mesure\n",
    "    y = A.dot(x)        #Les mesures\n",
    "    return A, y, x, n\n",
    "\n",
    "A, y, x, n = Init(500, 80, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code *Generesignal.py* génère un signal discret $x$ qui peut être vu comme une combinaison linéaire de cosinus à différentes fréquences et de diracs. Ce signal n'est pas parcimonieux dans la base canonique des diracs (car il faut à peu près $n$ diracs pour représenter un cosinus) et il n'est pas parcimonieux dans la base des sinus (il faut faire une combinaison linéaire de $n$ cosinus pour représenter un dirac).\n",
    "\n",
    "Par contre, ce signal est parcimonieux dans un frame qui est l'union de la base canonique et de la base des cosinus. \n",
    "Dans ce frame, il suffit en effet de $4$ coefficients non nuls pour reconstruire parfaitement le signal.\n",
    "\n",
    "> On choisira donc le frame représenté par une matrice $\\Psi=(I,C) \\in \\mathcal{ M}_{2n,n}(\\mathbb{R})$ o\\`u $C$ est une base de cosinus à différentes fréquences.\n",
    "\n",
    "### 4.1. Implémentation de l'itération proximale\n",
    "\n",
    "**Q6** Implémentez l'opérateur linéaire $\\Psi$ et son adjoint $\\Psi^*$. \n",
    "\n",
    "Pour $\\Psi$, vous vous servirez de la fonction $dct$ de Python dans la libraire scipy.fftpack qui calcule la transformée en cosinus discret d'un vecteur. Vous ferez attention à préciser *norm='ortho'* dans les options de la $dct$ pour que $idct$ soit bien l'opération inverse de $dct$.\n",
    "\n",
    "Pour $\\Psi^*$, vous utiliserez le fait que la $dct$ est une isométrie quand on précise \\textit{norm='ortho'} dans les options de $dct$.\n",
    "\n",
    "**R** \n",
    "$dct$ et $idct$ sont des isométries donc $(idct)^* = dct$. Ainsi, puisque $\\Psi(\\alpha)_i=\\alpha_i + idct((\\alpha_i)_{n+1\\leq i\\leq 2n})_{i+n} \\forall i \\in {1...n}$, on obtient :\n",
    "$$\\Psi^*=\\begin{pmatrix} I^* \\\\ C^*\\end{pmatrix}=\\begin{pmatrix} I \\\\ C^*\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear function Psi\n",
    "## (combination of sines and diracs) \n",
    "def Psi(alpha):\n",
    "    n = len(alpha) // 2\n",
    "    x = np.zeros(n)\n",
    "    x[:n] = alpha[:n] + fft.idct([alpha[n:]], norm='ortho', axis=-1, overwrite_x=False)\n",
    "    return x\n",
    "\n",
    "\n",
    "## The transpose of Psi\n",
    "def PsiT(x): \n",
    "    n = len(x)\n",
    "    alpha = np.zeros(2 * n)\n",
    "    alpha[:n] = x\n",
    "    alpha[n:] = fft.dct(x, norm='ortho', axis=-1, overwrite_x=False)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7** Implémentez l'algorithme proximal dans la fonction *RestoreX* avec les notations suivantes:\n",
    "* $A$ est la matrice d'échantillonnage.\n",
    "* $y$ est le vecteur de mesures.\n",
    "* $sigma$ est un paramètre du modèle.\n",
    "* $nit$ est le nombre d'itérations.\n",
    "* $alpha$ est la solution approximative du problème (4).\n",
    "* $x$ est donné par Psi $(\\alpha)$.\n",
    "* $CF$ est la fonction coût à chaque itération de l'algorithme. \n",
    "\n",
    "$$\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(\\alpha^k - \\frac{1}{L}\\nabla J(\\alpha^k)\\right).$$\n",
    "\n",
    "$$\n",
    "F(\\alpha)=\\|\\alpha\\|_1 + \\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla J(\\alpha) = \\sigma \\Psi^\\top A^\\top\\left(A\\Psi \\alpha - y\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prox of the l1−norm \n",
    "def prox(alpha, gamma):\n",
    "    return np.sign(alpha) * np.maximum(np.abs(alpha) - gamma, 0)\n",
    "\n",
    "def RestoreX(A, y, sigma, nit, xopt):\n",
    "    alpha = np.zeros(np.shape(A)[1] * 2) \n",
    "    L = 2 * sigma * npl.norm(A, 2) ** 2  # La norme triple de Psi vaut 2\n",
    "    CF = []\n",
    "    cv_rate = np.zeros(nit)\n",
    "    for i in range(nit):\n",
    "        x = Psi(alpha)\n",
    "        alpha = prox(alpha - 1 / L * sigma * PsiT(A.T.dot(A.dot(x) - y)), 1 / L)\n",
    "        CF.append(npl.norm(alpha, 1) + sigma / 2 * (npl.norm(np.dot(A, Psi(alpha)) - y)) ** 2)\n",
    "        cv_rate[i] = npl.norm(Psi(alpha) - xopt, 2) / npl.norm(x - xopt, 2)\n",
    "      \n",
    "    return alpha, x, CF, cv_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Testez votre algorithme ! Les paramètres $sigma$ et $nit$ sont des paramètres à choisir par vous-même (il faut en pratique beaucoup d'itérations pour converger). Vous pourrez observer la façon dont la suite $\\alpha^k$ se comporte au fur et à mesure des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sigma=widgets.FloatSlider(min=0.1, max=6, step=0.1, value=0.1, continuous_update=False), \n",
    "          nit=widgets.IntSlider(min=2500, max=50000, step=2500, value=2500, continuous_update=False))\n",
    "def test_sigma_nit(sigma, nit):\n",
    "    A, y, x, n = Init(500, 80)\n",
    "    fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
    "    alpha, xtilde, CF, cv_rate = RestoreX(A, y, sigma, nit, x)\n",
    "\n",
    "    ax[0, 0].plot(x)\n",
    "    ax[0, 0].set_title(\"Signal original\")\n",
    "\n",
    "    ax[0, 1].plot(xtilde)\n",
    "    ax[0, 1].set_title(\"Signal reconstruit\")\n",
    "    \n",
    "    ax[1, 0].plot(CF)\n",
    "    ax[1, 0].semilogy()\n",
    "    ax[1, 0].set_title(\"Fonction coût\")\n",
    "    \n",
    "    ax[1, 1].plot(alpha)\n",
    "    ax[1, 1].set_title(\"Alpha pour nit : \" + str(nit) + \", sigma : \" + str(sigma))\n",
    "    \n",
    "    ax[2,0].plot(cv_rate)\n",
    "    ax[2,0].set_title('Taux de convergence')\n",
    "    \n",
    "    ax[2,1].set_axis_off()\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Valeur finale de la fonction coût : \", CF[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma$ est le paramètre qui correspond à l'attachement aux données. Plus il est grand et plus on s'accroche aux données. \n",
    "Si on veut une forte accroche aux données, il faut augmenter considérablement le nombre d'itérations pour converger.\n",
    "\n",
    "On va visualiser l'évolution des coefficients $\\alpha_k$ et du signal reconstitué selon le nombre d'itérations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "sigma = 1\n",
    "p = 30\n",
    "nit = 15000\n",
    "A, y, x, n = Init(500, p)\n",
    "\n",
    "alpha = np.zeros(np.shape(A)[1] * 2) \n",
    "L = 2 * sigma * npl.norm(A, 2) ** 2  # La norme triple de Psi vaut 2\n",
    "CF = []\n",
    "x_plot = np.linspace(0, 1, num=np.shape(A)[1])\n",
    "\n",
    "plt.ion()  # Mode interactif on\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 10))\n",
    "ax[0].set_ylim((-4, 5))\n",
    "ax[1].set_ylim((-0.4, 0.4))\n",
    "\n",
    "for i in range(nit + 1):\n",
    "    x = Psi(alpha)\n",
    "    CF.append(npl.norm(alpha, 1) + sigma / 2 * (npl.norm(np.dot(A, x) - y)) ** 2)\n",
    "    alpha = prox(alpha - 1 / L * sigma * PsiT(A.T.dot(A.dot(Psi(alpha)) - y)), 1 / L)\n",
    "    \n",
    "    if i == 0:  # Initialisation du plot\n",
    "        line1, = ax[0].plot(range(alpha.shape[0]), alpha)\n",
    "        line2, = ax[1].plot(x_plot, x)\n",
    "        ax[0].set_title(\"Alpha à l'itération 1\")\n",
    "        ax[1].set_title(\"Signal reconstitué à l'itération 1\")\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(1)\n",
    "    elif i % 25 == 0:  # Refresh du signal toutes les 25 itérations\n",
    "        line1.set_ydata(alpha)\n",
    "        ax[0].set_title(\"Alpha à l'itération \" + str(i))\n",
    "        line2.set_ydata(x)\n",
    "        ax[1].set_title(\"Signal reconstitué à l'itération \" + str(i))\n",
    "        fig.canvas.draw()\n",
    "        fig.canvas.flush_events()\n",
    "        plt.pause(5 / i)\n",
    "               \n",
    "plt.ioff()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat attendu pour alpha est composé de 4 pics, c'est-à-dire 4 coefficients non nuls, deux correspondant aux cosinus et deux correspondant aux diracs.\n",
    "\n",
    "Sur cette animation on peut voir qu'à la première itération, tous les coefficients ont des valeurs non nulles mais proches de zéro. Ensuite, les coefficients à retrouver voient leur valeur augmenter, ce qui fait diminuer le terme $\\frac{\\sigma}{2} \\|A\\Psi\\alpha-y\\|_2^2$ de la fonction coût. Dans un second temps, on voit la valeur des autres coefficients se rapprocher de 0, ce qui minimise le terme $\\|\\alpha\\|_1$ de la fonction coût.\n",
    "\n",
    "On peut aussi visualiser l'influence du paramètre $\\sigma$ : s'il est trop petit ($\\sigma = 0.001$ pour $p = 30$), on échoue à reconstruire le signal, car la recherche de parcimonie du signal a trop d'influence face à l'attache aux données. On minimise donc la fonction coût en étant loin de la solution et en donnant des valeurs nulles aux coefficients. A l'inverse, une valeur de $\\sigma$ élevée va entraîner une convergence lente. En effet, on va alors chercher en priorité à s'approcher de la valeur mesurée au détriment de la recherche de la parcimonie.  \n",
    "\n",
    "L'intérêt de ce TP étant de reconstruire un signal avec le moins de mesures possibles, et le plus rapidement possible en terme d'itérations, choisir une petite valeur de $\\sigma$ semble être un bon compromis.\n",
    "\n",
    "**Q9**. Vérifiez que la fonction coût décroit de façon monotone. Quel est le taux de convergence observé ?\n",
    "\n",
    "Le taux de convergence est la suite des itérés $\\mu_k = \\frac{\\|x_{k+1} - x^*\\|}{\\|x_k - x^*\\|}$, où $x^*$ désigne la limite vers laquelle converge la suite $x_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, y, x, n = Init(500, 50)\n",
    "alpha, xtilde, CF, cv_rate = RestoreX(A, y, 0.3, 15000, x)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "ax[0].plot(CF)\n",
    "ax[0].semilogy()\n",
    "ax[0].set_title(\"Fonction cout\")\n",
    "ax[1].plot(cv_rate)\n",
    "ax[1].set_title(\"Taux de convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe un taux de convergence qui tend vers 1 : c'est une convergence sous-linéaire, c'est-à-dire lente. La fonction coût quant à elle décroit bien de façon monotone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** A partir de combien de mesures pouvez-vous reconstruire exactement le signal $x$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@interact(p=widgets.IntSlider(min=0, max=50, step=2, value=40, continuous_update=False), \n",
    "          sigma=widgets.FloatSlider(min=0.00, max=3, step=0.05, value=1, continuous_update=False))\n",
    "def Test_nb_Mesure(p, sigma):   \n",
    "    A, y, x, n = Init(500, p)\n",
    "    nit = 25000\n",
    "    alpha, xtilde, CF, cv_rate = RestoreX(A, y, sigma, nit, x)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.plot(alpha)\n",
    "    ax1.set_title(\"Coefficients alpha pour p = \" + str(p) + \" et sigma = \" + str(sigma))\n",
    "    ax2.plot(xtilde)\n",
    "    ax2.set_title(\"Signal reconstitué\")\n",
    "    plt.show()\n",
    "    dif = npl.norm(x - xtilde)\n",
    "    print(\"Différence entre le signal initial et reconstitué : \", dif)\n",
    "    print(\"Valeur finale de la fonction coût : \", CF[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe volontairement un `nit` assez grand (25000) car les itérations sont assez rapides à effectuer. En revanche si on peut diminuer considérablement le nombre de mesures effectuées (soit `p`), ça serait intéressant en pratique. On a fixé pour les tests un signal de 500 données au total.\n",
    "\n",
    "On peut reconstruire le signal pour $p = 40$. Il faut pour cela ajuster la valeur de $\\sigma$, et une valeur de 1 semble être un bon choix pour $nit = 25000$.\n",
    "\n",
    "Néanmoins, il semble préférable de réduire le nombre maximal d'itérations, et surtout le nombre de mesures. On peut donc envisager de réduire également la valeur de $\\sigma$ à $0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Implémentation de l'itération proximale accelérée\n",
    "\n",
    "On n'a a aucun moment utilisé la convexité de la fonction $J$ pour définir l'algorithme proximal. Celui-ci est de fait sous-optimal et peut être nettement accéléré. Dans notre cas, l'algorithme accéléré (Nesterov 2007) suit le schéma suivant:\n",
    "\n",
    "Paramètres en entrée:\n",
    "* $N$ le nombre d'itérations\n",
    "* $\\alpha^0\\in \\mathbb{R}^m$ un point initial.\n",
    "\n",
    "Algorithme\n",
    "> Poser $B^0=0_\\mathbb{R}$, $g^0=0_{\\mathbb{R}^m}$, $\\alpha=\\alpha^0$\n",
    "> For $k$= $0$ to $N$\n",
    "\n",
    ">> $t= \\frac{2}{L}$ \n",
    ">> $a^k = \\frac{1}{2}\\left(t+\\sqrt{t^2+4t B^k}\\right)$ \n",
    ">> $v^{k} =\\mbox{prox}_{B^k\\|\\cdot\\|_1}(\\alpha^0-g^k)$ \n",
    "\n",
    ">> $w^k = \\frac{B^k \\alpha^k +a^k v^k}{B^k+a^k}$ \n",
    "\n",
    ">>  $\\alpha^{k+1} = \\mbox{prox}_{\\frac{1}{L}\\|\\cdot\\|_1}\\left(w^k-\\frac{\\nabla J(w^k)}{L}\\right)$  \n",
    "\n",
    ">>\t$g^{k+1} = g^k + a^k\\nabla J(\\alpha^{k+1})$ \n",
    "\n",
    ">>\t$B^{k+1} = B^k+a^k$\n",
    " \n",
    "**Q11.** En vous aidant de ce que vous avez codé dans la partie précédente, implémentez cet algorithme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nesterov(A, y, sigma, nit, xopt):\n",
    "    B = 0\n",
    "    g = np.zeros(np.shape(A)[1] * 2)\n",
    "    alpha0 = np.zeros(np.shape(A)[1] * 2)\n",
    "    L = 2 * sigma * npl.norm(A, 2) ** 2\n",
    "    t = 2 / L\n",
    "    CF = []\n",
    "    alpha = np.zeros(np.shape(A)[1] * 2)\n",
    "    cv_rate = np.zeros(nit)\n",
    "    for k in range(nit):\n",
    "        a = 1 / 2 * (t + np.sqrt(t ** 2 + 4 * t * B))\n",
    "        v = prox(alpha - g, B)\n",
    "        w = (B * alpha + a * v) / (B + a)\n",
    "        grad_J_w =  sigma * PsiT(A.T.dot(A.dot(Psi(w)) - y))\n",
    "        xold = Psi(alpha)\n",
    "        alpha = prox(w - grad_J_w / L, 1 / L)\n",
    "        grad_J_alpha =  sigma * PsiT(A.T.dot(A.dot(Psi(alpha)) - y))\n",
    "        g += a * grad_J_alpha\n",
    "        B += a\n",
    "        CF.append(npl.norm(alpha, 1) + sigma / 2 * (npl.norm(np.dot(A, Psi(alpha)) - y)) ** 2)\n",
    "        cv_rate[k] = npl.norm(Psi(alpha) - xopt)/npl.norm(xold - xopt)\n",
    "    x = Psi(alpha)\n",
    "\n",
    "    return alpha, x, CF, cv_rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Testez le et comparez la rapidité d'execution de l'algorithme précédent et de celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sigma = 0.1\n",
    "p = 50\n",
    "A, y, x, n = Init(500, p)\n",
    "nit = 500\n",
    "\n",
    "alpha, xtilde, CF, cv_rate = Nesterov(A, y, sigma, nit, x)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "ax[0,0].plot(alpha)\n",
    "ax[0,0].set_title(\"Coefficients alpha pour p = \" + str(p) + \" et sigma = \" + str(sigma))\n",
    "\n",
    "ax[0,1].plot(xtilde)\n",
    "ax[0,1].set_title(\"Signal reconstitué\")\n",
    "\n",
    "ax[1,0].plot(CF)\n",
    "ax[1,0].semilogy()\n",
    "ax[1,0].set_title(\"Fonction coût\")\n",
    "\n",
    "ax[1,1].plot(cv_rate)\n",
    "ax[1,1].set_title(\"Taux de convergence\")\n",
    "\n",
    "plt.show()\n",
    "dif = npl.norm(x - xtilde)\n",
    "print(\"Différence entre le signal initial et reconstitué : \", dif)\n",
    "print(\"Valeur finale de la fonction coût : \", CF[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les graphiques ci-dessus sont obtenus pour $p = 50$ mesures, $\\sigma = 0.1$ et $nit = 500$ itérations.\n",
    "\n",
    "Le signal est bien reconstitué après un petit nombre d'itérations. \n",
    "Si on applique la première version de l'algorithme avec les mêmes paramètres, on observe qu'il n'a pas eu le temps de converger. La version accélérée est donc bien plus rapide. \n",
    "\n",
    "On remarque de plus que les paramètres optimaux sur ces deux algorithmes sont les mêmes. C'est tout à fait normal car l'algorithme accéléré se base sur les mêmes principes que l'algorithme proximal. Il prend juste en compte la convexité de notre fonction en plus.\n",
    "\n",
    "Les graphes ci-dessous montrent la reconstitution du signal pour les mêmes paramètres, mais avec la fonction `RestoreX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, y, x, n = Init(500, p)\n",
    "\n",
    "alpha, xtilde, CF, cv_rate = RestoreX(A, y, sigma, nit, x)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 15))\n",
    "ax[0,0].plot(alpha)\n",
    "ax[0,0].set_title(\"Coefficients alpha pour p = \" + str(p) + \" et sigma = \" + str(sigma))\n",
    "\n",
    "ax[0,1].plot(xtilde)\n",
    "ax[0,1].set_title(\"Signal reconstitué\")\n",
    "\n",
    "ax[1,0].plot(CF)\n",
    "ax[1,0].semilogy()\n",
    "ax[1,0].set_title(\"Fonction coût\")\n",
    "\n",
    "ax[1,1].plot(cv_rate)\n",
    "ax[1,1].set_title(\"Taux de convergence\")\n",
    "plt.show()\n",
    "dif = npl.norm(x - xtilde)\n",
    "print(\"Différence entre le signal initial et reconstitué : \", dif)\n",
    "print(\"Valeur finale de la fonction coût : \", CF[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Faites un rapide résumé des points qui vous ont semblé les plus importants dans ce TP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'échantillonnage compressif est un moyen efficace de contourner le théorème de Shannon lorsque l'on souhaite reconstituer convenablement un signal. En effet d'après ce théorème et avec le signal étudié, il aurait fallu que notre fréquence d'échantillonnage soit supérieure à 700 Hz, puisque notre signal d'origine était composé d'une sinusoïde de fréquence 350 Hz. Cela représente donc un nombre trop important de mesures à effectuer (700 au minimum) pour être utilisé en pratique. \n",
    "A l'inverse, grâce à l'échantillonnage compressif, on a pu reconstituer un signal avec un nombre très faible de mesures. De plus même si le signal était irrégulier (à cause des deux diracs), l'échantillonnage compressif s'est révélé efficace dans la reconstruction de ce signal. Avec nos paramètres choisis, on a pu se satisfaire de 40 mesures : c'est donc 17 fois moins que ce dont Shannon a besoin.\n",
    "\n",
    "La méthode accélérée est beaucoup plus efficace que la méthode de l'algorithme proximal classique. En effet, pour 50 mesures et $\\sigma = 0.1$, seulement 500 itérations suffisent à faire converger l'algorithme proximal accéléré. En revanche, les mêmes paramètres ne permettent pas du tout de faire converger l'algorithme classique.  On obtient de tels résultats avec `Nesterov` car cette méthode exploite la convexité de la fonction $J$, à défaut de pouvoir se baser sur sa différentiabilité.\n",
    "\n",
    "Le choix de $\\sigma$ demande plusieurs essais et dépend du choix de $p$. Il faut en effet faire un compromis en fonction du type de solution que l'on souhaite obtenir. Si on désire une solution précise, proche de la réalité, on pourra alors prendre un $\\sigma$ assez élevé, mais il faudra également augmenter le nombre d'itérations nécessaires à la reconstruction du signal. En revanche, si on souhaite calculer rapidement la solution, on peut se permettre de prendre $\\sigma$ petit, et ainsi réduire le nombre de mesures et le nombre d'itérations.\n",
    "\n",
    "Néanmoins, nous avons modifié la fonction coût dans le but de simplifier et d'accélérer la convergence de l'algorithme vers un point minimal de $\\| \\alpha_1 \\|$ sous la contrainte $A\\Psi\\alpha = y$ : c'est le problème $(4)$. On appelle cela la relaxation : on prend en compte la contrainte $A\\Psi\\alpha = y$ dans la fonction coût, mais de manière \"moins stricte\". La solution du problème $(4)$ ne tend vers une solution du problème initial que si $\\sigma$ tend vers $0$. En pratique, pour reconstituer notre signal on va donc choisir un $\\sigma$ faible, et ainsi diminuer considérablement le nombre de mesures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
